# DeepLearning_Keras

## An introduction to Deep Learning &amp; Neural Networks with Keras.

This repository provides a practical introduction to the foundational concepts of deep learning, using Keras to illustrate how to build and train neural networks effectively. It covers essential topics such as gradient descent, backpropagation, activation functions, and data preparation for classification tasks.

---

### Gradient Descent and the Training Loop

**Gradient Descent** is an iterative optimization algorithm used to minimize the model‚Äôs error. A key hyperparameter in this process is the **learning rate**:
- If the learning rate is **too high**, the model may overshoot the minimum and fail to converge.
- If it is **too low**, training becomes slow and may get stuck in suboptimal points.

Training a neural network involves repeatedly cycling through three main steps:
1. **Forward propagation** ‚Äì where the input is passed through the network to generate a prediction.
2. **Error calculation** ‚Äì comparing the predicted output with the true value.
3. **Backpropagation** ‚Äì updating weights and biases to reduce the error.

This loop continues over multiple iterations (called **epochs**) until a stopping criterion is met, such as a low enough error or a maximum number of epochs.

üìò Notebook:  [Backpropagation](https://github.com/ElsonFilho/DeepLearning_Keras/blob/main/notebooks/BackProp.ipynb)  

---

### ‚ö†Ô∏è Vanishing Gradient Problem

A significant challenge in training deep networks is the **vanishing gradient problem**, especially when using activation functions like **sigmoid**. In deep architectures:
- Gradients in early layers can become extremely small during backpropagation.
- This results in **slow or stalled learning** in those layers.
- Ultimately, this affects the model's ability to learn complex representations and degrades accuracy.

To mitigate this, activation functions that avoid shrinking gradients ‚Äî such as **ReLU** ‚Äî are preferred in modern architectures, especially in hidden layers.

---

### Activation Functions in Neural Networks

Activation functions introduce non-linearity into the network, enabling it to model complex relationships. Common types include:
- **Sigmoid**: Historically popular but now less used due to vanishing gradients.
- **Tanh**: A centered and scaled version of sigmoid, also prone to gradient issues.
- **ReLU (Rectified Linear Unit)**: The most widely used function today; it enables faster, more efficient training by avoiding activation of all neurons at once.
- **Softmax**: Used in the output layer for multi-class classification tasks, converting outputs into probability distributions.

üìò Notebook: [Activation Functions](https://github.com/ElsonFilho/DeepLearning_Keras/blob/main/notebooks/Activation_Functions.ipynb)

---

### Deep Learning Libraries: TensorFlow, PyTorch & Keras

There are several major libraries used for deep learning development:

- **TensorFlow**: Ideal for deploying models in production; backed by a strong community.
- **PyTorch**: Preferred in academic research; known for flexibility and strong GPU support.
- Both are powerful but can be challenging for beginners due to their complexity.

- **Keras**: A high-level API that simplifies the process of building and training deep learning models. It is especially beginner-friendly, offering:
  - Clean and readable syntax,
  - Rapid prototyping,
  - Integration with TensorFlow as its backend.

Keras enables the creation of powerful models with just a few lines of code, making it an excellent choice for those new to deep learning.

---

### Data Preparation for Keras Models

Before training a model with Keras, your dataset must be properly structured:
- Split your data into **predictors (input features)** and **target (labels)**.
- For **classification tasks**, the target values must be transformed into binary arrays using the `to_categorical()` function from Keras utilities.

Once prepared, the data can be fed into Keras models to build and train deep learning architectures efficiently.

üìò Notebook: [Intro to Keras](https://github.com/ElsonFilho/DeepLearning_Keras/blob/main/notebooks/Keras_Intro.ipynb)

---
